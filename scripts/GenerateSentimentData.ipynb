{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81eee8a8-69b8-4dc3-a050-4e20d580b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from os import listdir\n",
    "\n",
    "import SubtitleProcessing\n",
    "import VideoDownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6979415-0f9a-4c6a-a120-5d02fdf27c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_manual_labelling(subtitle_files, podcast_names, output_file_name, chunk_size, min_chunk_size = 10, filter_labels=None, filter_labels_proportions=None, guess_coin=True):\n",
    "    \"\"\"Prepares a csv file for manual sentiment labelling.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subtitle_files : list\n",
    "        Paths to all subtitle files to be used.\n",
    "    podcast_names : list\n",
    "        Names of the used podcasts. Ordering must match the subtitle_files list.\n",
    "    output_file_name : str\n",
    "        Name of the generated output file.\n",
    "    chunk_size : int\n",
    "        Word count of the chunks.\n",
    "    min_chunk_size : int\n",
    "        Chunks with less words will be discarded.\n",
    "    filter_labels : list\n",
    "        Keep text chunks auto labelled with these labels.\n",
    "    filter_labels_proportions : list\n",
    "        Determines how the resulting data will be proportioned based on the auto labels. If there is too much data for specific labels this data will be discarded.\n",
    "    guess_coin : bool\n",
    "        Already sets the 'Coin' field based on the Auto_Label\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(columns=[\"Podcast_Title\", \"Start_Time\", \"End_Time\", \"Auto_Label\", \"Text\", \"Coin\", \"Sentiment\"])\n",
    "    \n",
    "    for i in range(0,len(subtitle_files)):\n",
    "        # Get text chunks and their corresponding start and end times\n",
    "        text_chunks, chunk_start_times, chunk_end_times = SubtitleProcessing.generate_text_chunks(subtitle_files[i], chunk_size, min_chunk_size)\n",
    "        \n",
    "        if text_chunks is None:\n",
    "            continue\n",
    "        \n",
    "        # Fill the date in the dataframe\n",
    "        df_new = pd.DataFrame(columns=df.columns)\n",
    "        \n",
    "        text_chunks = [\" \".join(chunk) for chunk in text_chunks]\n",
    "        df_new[\"Text\"] = text_chunks\n",
    "        \n",
    "        df_new[\"Start_Time\"] = chunk_start_times\n",
    "        df_new[\"End_Time\"] = chunk_end_times\n",
    "        \n",
    "        df_new[\"Podcast_Title\"] = podcast_names[i]\n",
    "        \n",
    "        df_new[\"Auto_Label\"] = [SubtitleProcessing.auto_label_text_chunk_default_labels(t) for t in text_chunks]\n",
    "        \n",
    "        df = df.append(df_new, ignore_index=True)\n",
    "\n",
    "    # Filter and proportion data\n",
    "    if filter_labels is not None and filter_labels_proportions is not None:\n",
    "        df = filter_and_balance_by_auto_labels(df, filter_labels, filter_labels_proportions)\n",
    "    \n",
    "    if guess_coin:\n",
    "        df[\"Coin\"][df[\"Auto_Label\"].isin([\"BTC\",\"ETH\",\"DOGE\"])] = df[\"Auto_Label\"]\n",
    "\n",
    "    df.to_csv(output_file_name, index=False)\n",
    "\n",
    "\n",
    "def filter_by_auto_labels(df, filter_labels):\n",
    "    \"\"\"Only keeps rows where Auto_Label is in filter_labels\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return df[df[\"Auto_Label\"].isin(filter_labels)]\n",
    "\n",
    "\n",
    "def filter_and_balance_by_auto_labels(df, filter_labels, filter_labels_proportions):\n",
    "    \"\"\"Prepares a csv file for manual sentiment labelling.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Data.\n",
    "    filter_labels : list\n",
    "        Keep rows auto labelled with these labels.\n",
    "    filter_labels_proportions : list\n",
    "        Determines how the resulting data will be proportioned based on the auto labels. If there is too much data for specific labels this data will be discarded.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        return df\n",
    "    \n",
    "    df = filter_by_auto_labels(df, filter_labels)\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    label_least_data = filter_labels[0]  # The label with the least amount of data relative to the target proportion\n",
    "    label_least_data_current_percentage = 0\n",
    "    label_least_data_percentage_to_target = 100\n",
    "    \n",
    "    # Find the label with the least amount of data relative to its target proportion of data\n",
    "    for i in range(0,len(filter_labels)):\n",
    "        count = sum(df[\"Auto_Label\"] == filter_labels[i])\n",
    "        current_label_percentage = count / total_rows\n",
    "        current_label_percentage_to_target = current_label_percentage / filter_labels_proportions[i]\n",
    "        \n",
    "        if current_label_percentage_to_target < label_least_data_percentage_to_target:\n",
    "            label_least_data = i\n",
    "            label_least_data_current_percentage = current_label_percentage\n",
    "            label_least_data_percentage_to_target = current_label_percentage_to_target\n",
    "    \n",
    "    # Discard data of over represented labels\n",
    "    new_total_rows = sum(df[\"Auto_Label\"] == filter_labels[label_least_data]) / filter_labels_proportions[label_least_data]\n",
    "    for i in range(0,len(filter_labels)):\n",
    "        target_row_count = new_total_rows * filter_labels_proportions[i]  # Targeted row count of the current label\n",
    "        current_row_count = sum(df[\"Auto_Label\"] == filter_labels[i])\n",
    "        drop_rows = df[df[\"Auto_Label\"] == filter_labels[i]].sample(n=(current_row_count-int(target_row_count)))  # Pick random rows to drop\n",
    "\n",
    "        df = df.drop(drop_rows.index)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6040393b-a930-4642-ac71-628d5e11d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Test Sentiment Labelling Data\n",
    "\n",
    "folder_path = \"C:/Users/Tristan/nlp_project/podcast_data/labelling/\"\n",
    "podcast_names = [\"altcoin_daily_2021-06-01\",\n",
    "                \"altcoin_daily_2021-06-03\",\n",
    "                \"altcoin_daily_2021-06-04\",\n",
    "                \"altcoin_daily_2021-06-05\",\n",
    "                \"altcoin_daily_2021-06-06\",\n",
    "                \"altcoin_daily_2021-06-07\",\n",
    "                \"altcoin_daily_2021-06-08\",\n",
    "                \"altcoin_daily_2021-06-09\",\n",
    "                \"altcoin_daily_2021-06-10\",\n",
    "                \"altcoin_daily_2021-06-11\"]\n",
    "input_files = list()\n",
    "for name in podcast_names:\n",
    "    input_files.append(folder_path + name + \".vtt\")\n",
    "\n",
    "prepare_data_for_manual_labelling(input_files,\n",
    "                                  podcast_names,\n",
    "                                  \"sentiment_labels_altcoin_daily_210601_to_210611.csv\",\n",
    "                                  30,\n",
    "                                  filter_labels=[\"BTC\",\"ETH\",\"DOGE\",\"crypto_space\", \"None\"],\n",
    "                                  filter_labels_proportions=[.23,.23,.23,.23,.08])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bab025d-cd9d-41f6-8403-5f42b0e9b27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual word times are not supported for file: D:/nlp_project_d/podcast_data/labelling/batch_1//Altcoin Daily_20210419_BREAKING - Ethereum ETF Approved! United States of America Bitcoin ETF Update! Cryptocurrency News.en.vtt\n",
      "Could not generate text chunks for file: D:/nlp_project_d/podcast_data/labelling/batch_1//Altcoin Daily_20210419_BREAKING - Ethereum ETF Approved! United States of America Bitcoin ETF Update! Cryptocurrency News.en.vtt\n",
      "Individual word times are not supported for file: D:/nlp_project_d/podcast_data/labelling/batch_1//Altcoin Daily_20210505_Unilayer Platform Review_Walkthrough!_ DeFi in Cryptocurrency GROWING BIGGER!.en.vtt\n",
      "Could not generate text chunks for file: D:/nlp_project_d/podcast_data/labelling/batch_1//Altcoin Daily_20210505_Unilayer Platform Review_Walkthrough!_ DeFi in Cryptocurrency GROWING BIGGER!.en.vtt\n"
     ]
    }
   ],
   "source": [
    "# Generate Sentiment Labelling Data for Altcoin Daily 210401 to 210531 (due to bugs this batch was later downsized to 210401 to 210426)\n",
    "\n",
    "folder_path = \"D:/nlp_project_d/podcast_data/labelling/batch_1/\"\n",
    "all_file_names = listdir(folder_path)\n",
    "subtitle_file_paths = []\n",
    "podcast_names = []\n",
    "\n",
    "for file_name in all_file_names:\n",
    "    if file_name[-4:] == \".vtt\":\n",
    "        subtitle_file_paths.append(folder_path + '/' + file_name)\n",
    "        podcast_names.append(file_name)\n",
    "\n",
    "prepare_data_for_manual_labelling(subtitle_file_paths,\n",
    "                                  podcast_names,\n",
    "                                  \"sentiment_labels_altcoin_daily_210401_to_210531.csv\",\n",
    "                                  25,\n",
    "                                  filter_labels=[\"BTC\",\"ETH\",\"DOGE\",\"crypto_space\", \"None\"],\n",
    "                                  filter_labels_proportions=[.23,.23,.23,.23,.08])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f12245f1-0e2f-437d-bd84-c87ecc45d5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual word times are not supported for file: D:/nlp_project_d/podcast_data/labelling/batch_2//Altcoin Daily_20210505_Unilayer Platform Review_Walkthrough!_ DeFi in Cryptocurrency GROWING BIGGER!.en.vtt\n",
      "Could not generate text chunks for file: D:/nlp_project_d/podcast_data/labelling/batch_2//Altcoin Daily_20210505_Unilayer Platform Review_Walkthrough!_ DeFi in Cryptocurrency GROWING BIGGER!.en.vtt\n"
     ]
    }
   ],
   "source": [
    "# Generate Sentiment Labelling Data for Altcoin Daily 2104271 to 210531\n",
    "\n",
    "folder_path = \"D:/nlp_project_d/podcast_data/labelling/batch_2/\"\n",
    "all_file_names = listdir(folder_path)\n",
    "subtitle_file_paths = []\n",
    "podcast_names = []\n",
    "\n",
    "for file_name in all_file_names:\n",
    "    if file_name[-4:] == \".vtt\":\n",
    "        subtitle_file_paths.append(folder_path + '/' + file_name)\n",
    "        podcast_names.append(file_name)\n",
    "\n",
    "prepare_data_for_manual_labelling(subtitle_file_paths,\n",
    "                                  podcast_names,\n",
    "                                  \"sentiment_labels_altcoin_daily_210427_to_210531.csv\",\n",
    "                                  30,\n",
    "                                  filter_labels=[\"BTC\",\"ETH\",\"DOGE\",\"crypto_space\"],\n",
    "                                  filter_labels_proportions=[.25,.25,.25,.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b291f01-e917-432d-b740-0de90e1ea93f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b62ac34-3eb7-4c26-bae3-9ea48a7a4037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sentiment labelling data for \"This week in Blockchain\"\n",
    "\n",
    "folder_path = \"D:/nlp_project_d/podcast_data/labelling/batch_3\"\n",
    "\n",
    "VideoDownloader.ensure_correct_naming(folder_path)\n",
    "\n",
    "all_file_names = listdir(folder_path)\n",
    "subtitle_file_paths = []\n",
    "podcast_names = []\n",
    "\n",
    "for file_name in all_file_names:\n",
    "    if file_name[-4:] == \".vtt\":\n",
    "        subtitle_file_paths.append(folder_path + '/' + file_name)\n",
    "        podcast_names.append(file_name)\n",
    "\n",
    "prepare_data_for_manual_labelling(subtitle_file_paths,\n",
    "                                  podcast_names,\n",
    "                                  \"sentiment_labels_this_week_in_blockchain_210215_to_210621.csv\",\n",
    "                                  30,\n",
    "                                  filter_labels=[\"BTC\",\"ETH\",\"DOGE\",\"crypto_space\"],\n",
    "                                  filter_labels_proportions=[.33,.33,.01,.33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cfde74a-8eee-4f9f-9c25-095fe74fb4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find sentiment data generation bugs\n",
    "\n",
    "folder_path = \"D:/nlp_project_d/podcast_data/labelling/batch_1/\"\n",
    "subtitle_file_paths = []\n",
    "podcast_names = []\n",
    "\n",
    "for file_name in listdir(folder_path):\n",
    "    if file_name[-4:] == \".vtt\":\n",
    "        subtitle_file_paths.append(folder_path + '/' + file_name)\n",
    "        podcast_names.append(file_name)\n",
    "\n",
    "\n",
    "prepare_data_for_manual_labelling([subtitle_file_paths[0]],\n",
    "                                  [podcast_names[0]],\n",
    "                                  \"sentiment_labels_altcoin_daily_210401.csv\",\n",
    "                                  25)\n",
    "                                  #filter_labels=[\"BTC\",\"ETH\",\"DOGE\",\"crypto_space\", \"None\"],\n",
    "                                  #filter_labels_proportions=[.23,.23,.23,.23,.08])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ce416fe-f694-4cc8-b354-ae1e62570f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual word times are not supported for file: D:/nlp_project_d/podcast_data/labelling/batch_1//Altcoin Daily_20210419_BREAKING - Ethereum ETF Approved! United States of America Bitcoin ETF Update! Cryptocurrency News.en.vtt\n",
      "Could not generate text chunks for file: D:/nlp_project_d/podcast_data/labelling/batch_1//Altcoin Daily_20210419_BREAKING - Ethereum ETF Approved! United States of America Bitcoin ETF Update! Cryptocurrency News.en.vtt\n",
      "Individual word times are not supported for file: D:/nlp_project_d/podcast_data/labelling/batch_1//Altcoin Daily_20210505_Unilayer Platform Review_Walkthrough!_ DeFi in Cryptocurrency GROWING BIGGER!.en.vtt\n",
      "Could not generate text chunks for file: D:/nlp_project_d/podcast_data/labelling/batch_1//Altcoin Daily_20210505_Unilayer Platform Review_Walkthrough!_ DeFi in Cryptocurrency GROWING BIGGER!.en.vtt\n"
     ]
    }
   ],
   "source": [
    "# Fix timestamps\n",
    "\n",
    "df_old = pd.read_csv(\"C:/Users/Tristan/nlp_project/generated_data_for_labelling/sentiment_labels_altcoin_daily_210401_to_210531.csv\")\n",
    "\n",
    "df_corrected = pd.DataFrame(columns=[\"Start_Time_c\", \"End_Time_c\", \"Text\"])\n",
    "\n",
    "folder_path = \"D:/nlp_project_d/podcast_data/labelling/batch_1/\"\n",
    "all_file_names = listdir(folder_path)\n",
    "subtitle_file_paths = []\n",
    "podcast_names = []\n",
    "\n",
    "for file_name in all_file_names:\n",
    "    if file_name[-4:] == \".vtt\":\n",
    "        subtitle_file = folder_path + '/' + file_name\n",
    "        \n",
    "        text_chunks, chunk_start_times, chunk_end_times = SubtitleProcessing.generate_text_chunks(subtitle_file, 25, 10)\n",
    "        \n",
    "        if text_chunks is None:\n",
    "            continue\n",
    "        \n",
    "        df_c_new = pd.DataFrame(columns=df_corrected.columns)\n",
    "        \n",
    "        text_chunks = [\" \".join(chunk) for chunk in text_chunks]\n",
    "        df_c_new[\"Text\"] = text_chunks\n",
    "        \n",
    "        df_c_new[\"Start_Time_c\"] = chunk_start_times\n",
    "        df_c_new[\"End_Time_c\"] = chunk_end_times\n",
    "\n",
    "        df_corrected = df_corrected.append(df_c_new, ignore_index=True) \n",
    "\n",
    "df_join = df_old.join(df_corrected.set_index(\"Text\"), on=\"Text\")\n",
    "\n",
    "df_join.to_csv(\"corrected_timestamps.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b424406e-71bd-446b-97e5-5045fc52bde2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
