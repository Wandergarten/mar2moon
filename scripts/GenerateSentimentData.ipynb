{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81eee8a8-69b8-4dc3-a050-4e20d580b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import SubtitleProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6979415-0f9a-4c6a-a120-5d02fdf27c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_manual_labelling(subtitle_files, podcast_names, output_file_name, chunk_size, min_chunk_size = 10):\n",
    "    \"\"\"Prepares a csv file for manual sentiment labelling.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subtitle_files : list\n",
    "        Paths to all subtitle files to be used.\n",
    "    podcast_names : list\n",
    "        Names of the used podcasts. Ordering must match the subtitle_files list.\n",
    "    output_file_name : str\n",
    "        Name of the generated output file.\n",
    "    chunk_size : int\n",
    "        Word count of the chunks.\n",
    "    min_chunk_size : int\n",
    "        Chunks with less words will be discarded.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(columns=[\"Podcast_Title\", \"Start_Time\", \"End_Time\", \"Text\", \"Coin\", \"Sentiment\"])\n",
    "    \n",
    "    for i in range(0,len(subtitle_files)):\n",
    "        text_chunks, chunk_start_times, chunk_end_times = SubtitleProcessing.generate_text_chunks(subtitle_files[i], chunk_size, min_chunk_size)\n",
    "        \n",
    "        df_new = pd.DataFrame(columns=df.columns)\n",
    "        \n",
    "        text_chunks = [\" \".join(chunk) for chunk in text_chunks]\n",
    "        df_new[\"Text\"] = text_chunks\n",
    "        \n",
    "        df_new[\"Start_Time\"] = chunk_start_times\n",
    "        df_new[\"End_Time\"] = chunk_end_times\n",
    "        \n",
    "        df_new[\"Podcast_Title\"] = podcast_names[i]\n",
    "        \n",
    "        df = df.append(df_new, ignore_index=True)\n",
    "    \n",
    "    df.to_csv(output_file_name, index=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6040393b-a930-4642-ac71-628d5e11d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"C:/Users/Tristan/nlp_project/podcast_data/labelling/\"\n",
    "podcast_names = [\"altcoin_daily_2021-06-01\",\n",
    "                \"altcoin_daily_2021-06-03\",\n",
    "                \"altcoin_daily_2021-06-04\",\n",
    "                \"altcoin_daily_2021-06-05\",\n",
    "                \"altcoin_daily_2021-06-06\",\n",
    "                \"altcoin_daily_2021-06-07\",\n",
    "                \"altcoin_daily_2021-06-08\",\n",
    "                \"altcoin_daily_2021-06-09\",\n",
    "                \"altcoin_daily_2021-06-10\",\n",
    "                \"altcoin_daily_2021-06-11\"]\n",
    "input_files = list()\n",
    "for name in podcast_names:\n",
    "    input_files.append(folder_path + name + \".vtt\")\n",
    "\n",
    "prepare_data_for_manual_labelling(input_files, podcast_names, \"sentiment_labels_altcoin_daily_210601_to_210611.csv\", 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
