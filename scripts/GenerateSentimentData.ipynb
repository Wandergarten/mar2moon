{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "81eee8a8-69b8-4dc3-a050-4e20d580b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "d6979415-0f9a-4c6a-a120-5d02fdf27c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_manual_labelling(subtitle_files, podcast_names, output_file_name, chunk_size, min_chunk_size = 10):\n",
    "    \"\"\"Prepares a csv file for manual sentiment labelling.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subtitle_files : list\n",
    "        Paths to all subtitle files to be used.\n",
    "    podcast_names : list\n",
    "        Names of the used podcasts. Ordering must match the subtitle_files list.\n",
    "    output_file_name : str\n",
    "        Name of the generated output file.\n",
    "    chunk_size : int\n",
    "        Word count of the chunks.\n",
    "    min_chunk_size : int\n",
    "        Chunks with less words will be discarded.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(columns=[\"Podcast_Title\", \"Start_Time\", \"End_Time\", \"Text\", \"Coin\", \"Sentiment\"])\n",
    "    \n",
    "    for i in range(0,len(subtitle_files)):\n",
    "        words, word_end_times = get_words_with_end_times(subtitle_files[i])\n",
    "\n",
    "        # Generate text chunks of desired size\n",
    "        text_chunks = list()\n",
    "        chunk_start_times = list()\n",
    "        chunk_end_times = list()\n",
    "        for j in range(0, len(words), chunk_size):\n",
    "            text_chunks.append(words[j:j + chunk_size])\n",
    "            \n",
    "            # Save start time\n",
    "            if len(chunk_end_times) == 0:\n",
    "                chunk_start_times.append(\"00:00:00.000\")\n",
    "            else:\n",
    "                chunk_start_times.append(chunk_end_times[-1])\n",
    "                \n",
    "            # Save end time\n",
    "            chunk_end_times.append(word_end_times[min(j + chunk_size, len(word_end_times)-1)])\n",
    "        \n",
    "        # Discard last chunk if too small\n",
    "        if(len(text_chunks[-1]) < min_chunk_size):\n",
    "            text_chunks.pop()\n",
    "            chunk_start_times.pop()\n",
    "            chunk_end_times.pop()\n",
    "        \n",
    "        \n",
    "        text_chunks = [\" \".join(chunk) for chunk in text_chunks]\n",
    "        df[\"Text\"] = text_chunks\n",
    "        \n",
    "        df[\"Start_Time\"] = chunk_start_times\n",
    "        df[\"End_Time\"] = chunk_end_times\n",
    "        \n",
    "        df[\"Podcast_Title\"] = podcast_names[i]\n",
    "    \n",
    "    \n",
    "        df.to_csv(output_file_name, index=False)\n",
    "            \n",
    "\n",
    "def get_words_with_end_times(subtitle_file):\n",
    "    \"\"\"Get all words from a subtitle file (vtt format) with their corresponding end timestamps\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    with open(subtitle_file) as subtitle_file:\n",
    "            \n",
    "            # Remove first 4 lines (containing meta information)\n",
    "            for j in range(0,4):\n",
    "                subtitle_file.readline()\n",
    "            \n",
    "            text = subtitle_file.read()\n",
    "            \n",
    "            chunks = text.split(\" \\n\\n\") #  split into chunks for easier data processing\n",
    "            \n",
    "            words = list()\n",
    "            word_end_times = list()\n",
    "            \n",
    "            for chunk in chunks:\n",
    "                chunk_lines = chunk.split(\"\\n\")\n",
    "                words_line = chunk_lines[2]\n",
    "                \n",
    "                first_word_end_index = words_line.find(\"<\")\n",
    "                if(first_word_end_index != -1):\n",
    "                    first_word = words_line[0:first_word_end_index]  # get the first word (can't be found using method below)\n",
    "                    \n",
    "                    words_in_chunk = re.findall(\"<c> [\\S]*</c>\", words_line)  # get all words\n",
    "                    words_in_chunk = [w[4:-4] for w in words_in_chunk]  # strip <c> and <c/>\n",
    "                    \n",
    "                    word_end_times_in_chunk = re.findall(\"<\\d\\d:\\d\\d:\\d\\d.\\d\\d\\d>\", words_line)  # get all word end times\n",
    "                    word_end_times_in_chunk = [t[1:-1] for t in word_end_times_in_chunk]  # strip < and >\n",
    "                else:\n",
    "                    # Only one word\n",
    "                    first_word = words_line\n",
    "                    \n",
    "                last_time = chunk_lines[4][17:29]  # end time for the last word\n",
    "                \n",
    "                words_in_chunk.insert(0, first_word)\n",
    "                word_end_times_in_chunk.append(last_time)\n",
    "                \n",
    "                words.extend(words_in_chunk)\n",
    "                word_end_times.extend(word_end_times_in_chunk)\n",
    "                \n",
    "            # For the last chunk we have to get the word end time from somewhere else\n",
    "            first_line_in_last_chunk = chunks[-1].split(\"\\n\")[0]\n",
    "            last_time = first_line_in_last_chunk[17:29]\n",
    "            word_end_times.pop()\n",
    "            word_end_times.append(last_time)\n",
    "            \n",
    "            if len(words) != len(word_end_times):\n",
    "                print(\"Warning: word count does not match times count\")\n",
    "            \n",
    "            return words, word_end_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "6040393b-a930-4642-ac71-628d5e11d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"C:/Users/Tristan/nlp_project/podcast_data/Test/\"\n",
    "podcast_names = [\"altcoin_daily_test\"]\n",
    "input_files = list()\n",
    "for name in podcast_names:\n",
    "    input_files.append(folder_path + name + \".vtt\")\n",
    "\n",
    "prepare_data_for_manual_labelling(input_files, podcast_names, \"sent_label_test.csv\", 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
