{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81eee8a8-69b8-4dc3-a050-4e20d580b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import SubtitleProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d6979415-0f9a-4c6a-a120-5d02fdf27c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_manual_labelling(subtitle_files, podcast_names, output_file_name, chunk_size, min_chunk_size = 10, filter_labels=None, filter_labels_proportions=None, guess_coin=True):\n",
    "    \"\"\"Prepares a csv file for manual sentiment labelling.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subtitle_files : list\n",
    "        Paths to all subtitle files to be used.\n",
    "    podcast_names : list\n",
    "        Names of the used podcasts. Ordering must match the subtitle_files list.\n",
    "    output_file_name : str\n",
    "        Name of the generated output file.\n",
    "    chunk_size : int\n",
    "        Word count of the chunks.\n",
    "    min_chunk_size : int\n",
    "        Chunks with less words will be discarded.\n",
    "    filter_labels : list\n",
    "        Keep text chunks auto labelled with these labels.\n",
    "    filter_labels_proportions : list\n",
    "        Determines how the resulting data will be proportioned based on the auto labels. If there is too much data for specific labels this data will be discarded.\n",
    "    guess_coin : bool\n",
    "        Already sets the 'Coin' field based on the Auto_Label\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(columns=[\"Podcast_Title\", \"Start_Time\", \"End_Time\", \"Auto_Label\", \"Text\", \"Coin\", \"Sentiment\"])\n",
    "    \n",
    "    for i in range(0,len(subtitle_files)):\n",
    "        # Get text chunks and their corresponding start and end times\n",
    "        text_chunks, chunk_start_times, chunk_end_times = SubtitleProcessing.generate_text_chunks(subtitle_files[i], chunk_size, min_chunk_size)\n",
    "        \n",
    "        \n",
    "        # Fill the date in the dataframe\n",
    "        df_new = pd.DataFrame(columns=df.columns)\n",
    "        \n",
    "        text_chunks = [\" \".join(chunk) for chunk in text_chunks]\n",
    "        df_new[\"Text\"] = text_chunks\n",
    "        \n",
    "        df_new[\"Start_Time\"] = chunk_start_times\n",
    "        df_new[\"End_Time\"] = chunk_end_times\n",
    "        \n",
    "        df_new[\"Podcast_Title\"] = podcast_names[i]\n",
    "        \n",
    "        df_new[\"Auto_Label\"] = [SubtitleProcessing.auto_label_text_chunk_default_labels(t) for t in text_chunks]\n",
    "        \n",
    "        df = df.append(df_new, ignore_index=True)\n",
    "    \n",
    "    # Filter and proportion data\n",
    "    df = filter_and_balance_by_auto_labels(df, filter_labels, filter_labels_proportions)\n",
    "    \n",
    "    if guess_coin:\n",
    "        df[\"Coin\"][df[\"Auto_Label\"].isin([\"BTC\",\"ETH\",\"DOGE\"])] = df[\"Auto_Label\"]\n",
    "\n",
    "    df.to_csv(output_file_name, index=False)\n",
    "\n",
    "\n",
    "def filter_by_auto_labels(df, filter_labels):\n",
    "    \"\"\"Only keeps rows where Auto_Label is in filter_labels\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return df[df[\"Auto_Label\"].isin(filter_labels)]\n",
    "\n",
    "\n",
    "def filter_and_balance_by_auto_labels(df, filter_labels, filter_labels_proportions):\n",
    "    \"\"\"Prepares a csv file for manual sentiment labelling.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Data.\n",
    "    filter_labels : list\n",
    "        Keep rows auto labelled with these labels.\n",
    "    filter_labels_proportions : list\n",
    "        Determines how the resulting data will be proportioned based on the auto labels. If there is too much data for specific labels this data will be discarded.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = filter_by_auto_labels(df, filter_labels)\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    label_least_data = filter_labels[0]  # The label with the least amount of data relative to the target proportion\n",
    "    label_least_data_current_percentage = 0\n",
    "    label_least_data_percentage_to_target = 100\n",
    "    \n",
    "    # Find the label with the least amount of data relative to its target proportion of data\n",
    "    for i in range(0,len(filter_labels)):\n",
    "        count = sum(df[\"Auto_Label\"] == filter_labels[i])\n",
    "        current_label_percentage = count / total_rows\n",
    "        current_label_percentage_to_target = current_label_percentage / filter_labels_proportions[i]\n",
    "        \n",
    "        if current_label_percentage_to_target < label_least_data_percentage_to_target:\n",
    "            label_least_data = i\n",
    "            label_least_data_current_percentage = current_label_percentage\n",
    "            label_least_data_percentage_to_target = current_label_percentage_to_target\n",
    "    \n",
    "    # Discard data of over represented labels\n",
    "    new_total_rows = sum(df[\"Auto_Label\"] == filter_labels[label_least_data]) / filter_labels_proportions[label_least_data]\n",
    "    for i in range(0,len(filter_labels)):\n",
    "        target_row_count = new_total_rows * filter_labels_proportions[i]  # Targeted row count of the current label\n",
    "        current_row_count = sum(df[\"Auto_Label\"] == filter_labels[i])\n",
    "        drop_rows = df[df[\"Auto_Label\"] == filter_labels[i]].sample(n=(current_row_count-int(target_row_count)))  # Pick random rows to drop\n",
    "\n",
    "        df = df.drop(drop_rows.index)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6040393b-a930-4642-ac71-628d5e11d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"C:/Users/Tristan/nlp_project/podcast_data/labelling/\"\n",
    "podcast_names = [\"altcoin_daily_2021-06-01\",\n",
    "                \"altcoin_daily_2021-06-03\",\n",
    "                \"altcoin_daily_2021-06-04\",\n",
    "                \"altcoin_daily_2021-06-05\",\n",
    "                \"altcoin_daily_2021-06-06\",\n",
    "                \"altcoin_daily_2021-06-07\",\n",
    "                \"altcoin_daily_2021-06-08\",\n",
    "                \"altcoin_daily_2021-06-09\",\n",
    "                \"altcoin_daily_2021-06-10\",\n",
    "                \"altcoin_daily_2021-06-11\"]\n",
    "input_files = list()\n",
    "for name in podcast_names:\n",
    "    input_files.append(folder_path + name + \".vtt\")\n",
    "\n",
    "prepare_data_for_manual_labelling(input_files,\n",
    "                                  podcast_names,\n",
    "                                  \"sentiment_labels_altcoin_daily_210601_to_210611.csv\",\n",
    "                                  30,\n",
    "                                  filter_labels=[\"BTC\",\"ETH\",\"DOGE\",\"crypto_space\", \"None\"],\n",
    "                                  filter_labels_proportions=[.23,.23,.23,.23,.08])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb728dd-6772-465a-8174-be490d3cd52a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
